{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We give a brief introduction to the course.\n",
    "\n",
    "We then present the key concepts that form the basis for this course\n",
    "- For some: this will be review\n",
    "- For others: it will be a preview\n",
    "\n",
    "## Intro to Advanced Course\n",
    "\n",
    "- [Introduction to Advanced Course](Intro_Advanced.ipynb)\n",
    "- [Review and Preview](Review_Advanced.ipynb)\n",
    "\n",
    "\n",
    "You may want to run your code on Google Colab in order to take advantage of powerful GPU's.\n",
    "\n",
    "Here are some useful tips:\n",
    "\n",
    "[Google Colab tricks](Colab_practical.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review/Preview of concepts from Intro Course\n",
    "\n",
    "### [Transfer Learning: Review ](Review_TransferLearning.ipynb)\n",
    "  \n",
    "### [Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- Attention\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "    \n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - Geron Chapter 16\n",
    "    - [An Analysis of BERT's Attention](https://arxiv.org/pdf/1906.04341.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Review (continued)\n",
    "\n",
    "**Preview**\n",
    "\n",
    "There is lots of interest in Large Language Models (e.g., ChatGPT).  These are based on an architecture called the Transformer.  We will introduce the Transformer and demonstrate some amazing results achieved by using Transformers to create Large Language Models.\n",
    "\n",
    "Attention is a mechanism that is a core part of the Transformer.  We will begin by first introducing Attention.\n",
    "\n",
    "We will then take a detour and study the Functional model architecture of Keras.  Unlike the Sequential model, which is an ordered sequence of Layers, the organization of blocks in a Functional model is more general.  The Advanced architectures (e.g., the Transformer) are built using the Functional model.\n",
    "\n",
    "Once we understand the technical prerequisites, we will examine the code for the Transformer.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue the review/preview of key concepts that we started last week.\n",
    "- We will *re-start* the module on Attention\n",
    "\n",
    "Our ultimate goal is to introduce the Transformer (which uses Attention heavily) in theory, and demonstrate its use in Large Language Models.\n",
    "\n",
    "## Review continued\n",
    "\n",
    "### [Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "### [Natural Language Processing: Review](Review_NLP.ipynb)\n",
    "\n",
    "### [Language Models, the future (present ?) of NLP: Review](Review_LLM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3: Technical\n",
    "\n",
    "**Plan (part 1)**\n",
    "\n",
    "We finish up the Language Model module with some surprising abilities that seem to \"emerge\" when LLM's become very large.\n",
    "\n",
    "[In context learning](Review_LLM.ipynb#In-context-Learning)\n",
    "\n",
    "Here is a very crude notebook that uses the HuggingFace inference API to experiment with in-context learning.\n",
    "\n",
    "- [Experiment in In context learning: Colab](https://colab.research.google.com/github/kenperry-public/ML_Advanced_Spring_2023/blob/master/HF_inference_play.ipynb)\n",
    "- [Experiment in In context learning: local](HF_inference_play.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Transfer Learning: Fine-tuning a pre-trained model\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We introduce \"Modern Transfer Learning\": using model hubs.\n",
    "\n",
    "The hub we will use for the final project: HuggingFace\n",
    "- illustrate how to fine-tune a pre-trained model\n",
    "- quick Intro to HF\n",
    "    - best way to learn: through the course !\n",
    "    - uses Datasets\n",
    "        - will introduce later\n",
    "    - PyTorch version (uses Trainer); we will focus on Tensorflow/Keras version\n",
    "\n",
    "**HuggingFace Transformers course**\n",
    "\n",
    "The best way to understand and use modern Transfer Learning is via\n",
    "the [HuggingFace course](https://huggingface.co/course).\n",
    "\n",
    "You will learn\n",
    "- about the Transformer\n",
    "- how to use HuggingFace's tools for NLP (e.g., Tokenizers)\n",
    "- how to perform common NLP tasks\n",
    "    - especially with Transformers\n",
    "- how to fine-tune a pre-trained model\n",
    "- how to use the HuggingFace dataset API\n",
    "\n",
    "All of this will be invaluable for the Course Project.\n",
    "- does not have to be done using HuggingFace\n",
    "- but using at least parts of it will make your task easier\n",
    "\n",
    "\n",
    "- [HuggingFace intro](Transfer_Learning_HF.ipynb)\n",
    "    - [linked notebook: Using a pretrained Sequence Classifier](HF_quick_intro_to_models.ipynb) \n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "[HuggingFace course](https://huggingface.co/course)\n",
    "- you are well-advised to follow this material over the next 3 weeks in preparation for the Course Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Functional Models\n",
    "\n",
    "**Plan (part 2)**\n",
    "\n",
    "Enough theory (for the moment) !\n",
    "\n",
    "The Transformer (whose theory we have presented) is built from plain Keras.\n",
    "\n",
    "Our goal is to dig into the **code** for the Transformer so that you too will learn how to build advanced models.\n",
    "\n",
    "Before we can do this, we must\n",
    "- go beyond the Sequential model of Keras: introduction to the Functional model\n",
    "- understand more \"advanced\" features of Keras: customomizing layers,  training loops, loss functions\n",
    "- The Datasets API\n",
    "\n",
    "**Basics**\n",
    "\n",
    "We start with the basics of Functional models, and will give a coding example of such a model in Finance.\n",
    "\n",
    "- [Functional API](Functional_Models.ipynb)\n",
    "\n",
    "\n",
    "### Functional Model Code:  A Functional model in Finance: \"Factor model\"\n",
    "\n",
    "We illustrate the basic features of Functional models with an example\n",
    "- does not use the additional techniques of the next section (Advanced Keras)\n",
    "\n",
    "[Autoencoders for Conditional Risk Factors](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue our exploration of the Functional API in Keras.\n",
    "\n",
    "We will spend some time examining the code for the Transformer.\n",
    "\n",
    "We will also introduce the TensorFlow Dataset (TFDS) API, a way to consume large datasets using a limited amount of memory.\n",
    "\n",
    "THIS WILL BE A VERY CODE-INTENSIVE WEEK\n",
    "\n",
    "## Functional Model Code:  A Functional model in Finance: \"Factor model\"\n",
    "\n",
    "We finish up from last week by looking at the actual code\n",
    "\n",
    "[Autoencoders for Conditional Risk Factors](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "\n",
    "## Datasets: Big data in small memory\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Last piece of technical info to enable the project\n",
    "\n",
    "- [TensorFlow Dataset](TF_Data_API.ipynb)\n",
    "\n",
    "**Background**\n",
    "- [Python generators](Generators.ipynb)\n",
    "\n",
    "**Notebooks**\n",
    "- [Dataset API: play around](TFDatasets_play_v1.ipynb)\n",
    "\n",
    "## Advanced Keras\n",
    "\n",
    "Keras provides many features that make it easier to write complex models\n",
    "- Custom layer types\n",
    "- Custom training loops\n",
    "- Custom Loss functions\n",
    "\n",
    "We will illustrate these techniques with a coding example.\n",
    "\n",
    "- [Multiple models combined: Transformer](Keras_Advanced.ipynb#Functional-model:-the-basics,-illustrated-by-the-Transformer)\n",
    "- [Custom layers](Keras_Advanced.ipynb#Custom-layers:-subtle-point)\n",
    "- [Custom loss, Custom training loop](Keras_Advanced.ipynb#Model-specialization)\n",
    "\n",
    "\n",
    "### Advanced Keras code:  Neural Style Transfer: Non-trivial Loss function\n",
    "\n",
    "- [Neural Style Transfer](Neural_Style_Transfer.ipynb)\n",
    "\n",
    "## Putting it all together: Code: the Transformer\n",
    "\n",
    "We now have enough background to understand the code for the Transformer.\n",
    "\n",
    "We will examine the code in the excellent [TensorFlow tutorial on the Transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "[The Transformer: Code](Transformer_code.ipynb)\n",
    "\n",
    "**Suggest reading**\n",
    "\n",
    "[Tensorflow tutorial: Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "\n",
    "## Attention in detail\n",
    "\n",
    "In our review, we had deferred a detailed view of implementing Attention. Now we will\n",
    "explore it at a very hight level.\n",
    "\n",
    "We *will not* spend time on the actual code.  If you're interested there are several web articles\n",
    "that do so, for example, [here](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)\n",
    "\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "\n",
    "## Functional models: subtleties\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "- [Eager vs Graph Execution](TF_Graph.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "## Putting it all together: Code: the Transformer (continued)\n",
    "\n",
    "We almost finished the module last week.  Let's wrap up with some details related to Training.\n",
    "\n",
    "[The Transformer: Code (continued)](Transformer_code.ipynb#Custom-Learning-Rate-Schedule)\n",
    "\n",
    "\n",
    "## Synthetic Data: Autoencoders\n",
    "\n",
    "New major topic: Synthetic data.\n",
    "\n",
    "After last week's \"code-heavy\" modules, we are back to \"theory\" !\n",
    "\n",
    "We will address several ways to create new examples, staring with the simplest model and moving on to models that are more complex.\n",
    "\n",
    "\n",
    "Generating synthetic data using Autoencoders and its variants.\n",
    "\n",
    "### \"Vanilla\" Autoencoder\n",
    "\n",
    "[Autoencoder](Autoencoders_Generative.ipynb)\n",
    "\n",
    "**Suggested Reading**\n",
    "\n",
    "[TensorFlow Tutorial on Autoencoders](https://www.tensorflow.org/tutorials/generative/autoencoder)\n",
    "\n",
    "\n",
    "### Variational Autoencoder (VAE)\n",
    "\n",
    "\n",
    "We now study a different type of Autoencoder\n",
    "- that learns a *distribution* over the training examples\n",
    "- by sampling from this distribution: we can create synthetic examples\n",
    "\n",
    "[Variational Autoeconder (VAE)](VAE_Generative.ipynb)\n",
    "\n",
    "**Suggested Reading**\n",
    "\n",
    "[TensorFlow tutorial on VAE](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "[Tutorial on VAE](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 Advanced Topics\n",
    "\n",
    "Topics with a  <font style=\"background-color:MediumSeaGreen;\">Green Background</font>\n",
    "have been \"up-voted\" in a survey of students.\n",
    "\n",
    "## Synthetic Data: Vector Quantized Autoencoders\n",
    "\n",
    "While we are on the topic of Autoencoders, we present the Vector Quantized Autoencoder. \n",
    "\n",
    "It is not so much a tool for creating Synthetic Data as a natural continuation of our Autoencoder exploration.\n",
    "\n",
    "[Vector Quantized Autoencoder](VQ_VAE_Generative.ipynb)\n",
    "\n",
    "**Suggested Reading**\n",
    "\n",
    "[vanilla VQ-VAE](https://arxiv.org/pdf/1711.00937.pdf)\n",
    "\n",
    "[VQ-VAE-2 paper](https://arxiv.org/pdf/1906.00446.pdf)\n",
    "\n",
    "\n",
    "## Synthetic Data: Self-improvement of an LLM by generating examples \n",
    "\n",
    "We present a way of synthesizing examples to improve a Large Language Model.  In this case: the examples we create are *text*.\n",
    "\n",
    "This is a potential solution to one of the issues with Fine-Tuning a LLM: the lack of sufficient\n",
    "labeled examples for the Target task.\n",
    "\n",
    "[LLM Self Improvement](LLM_Self_Instruction.ipynb)\n",
    "\n",
    "**Suggested Reading**\n",
    "- [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf)\n",
    "- [Self improvement](https://arxiv.org/pdf/2210.11610.pdf)\n",
    "    - goal is to fine-tune a LLM for question answering\n",
    "        - without an **a priori** fine-tuning dataset\n",
    "   - use a LLM to **generate** a fine-tuning dataset\n",
    "       - Use few-shot, CoT prompts: \n",
    "           - Input=question; Output=answer + rationale\n",
    "           - Input=question, LLM generates output\n",
    "               - multiple outputs\n",
    "               - extract answer from output\n",
    "                   - use majority voting on answer to filter responses\n",
    "                   - hopefully: majority is accurate: \"high confidence\" == fraction of responses that agree ?\n",
    "       - The high confidence (large fraction of generated responses to a question agree in answer) generated examples become the fine-tuning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Synthetic Data: GANs\n",
    "\n",
    "We introduce a new type of model that can be used to generate synthetic data: the Generative Adversarial Network (GAN).  It uses a competitive process involving two Neural Networks in order to iteratively\n",
    "produce synthetic examples of increasing fidelity to the true data.\n",
    "\n",
    "- [GAN: basic](GAN_Generative.ipynb)\n",
    "- [GAN loss](GAN_Loss_Generative.ipynb)\n",
    "- [Wasserstein GAN](Wasserstein_GAN_Generative.ipynb)\n",
    "\n",
    "**Notebooks**\n",
    "- [GAN to Generate Faces](CelebA_01_deep_convolutional_generative_adversarial_network.ipynb)\n",
    "    \n",
    "**Suggested reading**\n",
    "- [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)\n",
    "- [TensorFlow Tutorial DCGAN](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb)\n",
    "    - this is a tutorial from which our code notebook was derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  <font style=\"background-color:MediumSeaGreen;\">Synthetic Data: GANs for Timeseries (deferred)</font>\n",
    "\n",
    "**Timeseries data**\n",
    "- [Time GAN](TimeGAN_Generative.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- [TimeGAN](https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n",
    "    - [supplement](https://www.vanderschaar-lab.com/papers/NIPS2019_TGAN_Supplementary.pdf)\n",
    "    - [github](https://github.com/vanderschaarlab/mlforhealthlabpub/tree/main/alg/timegan)\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "[Quant GAN](https://arxiv.org/pdf/1907.06673.pdf)\n",
    "- Similar goal as TimeGAN\n",
    "    - Uses *Temporal Convolution Networks (TCN)* for Generator and Discriminator\n",
    "        - Is Dilated Convolution\n",
    "        - effectively creates a longer window\n",
    "    - [Quant GAN used for Risk Management: lecture slides](https://cfe.columbia.edu/sites/default/files/content/slides/Modelling%20Asset%20Returns%20with%20Quant%20GANs.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  <font style=\"background-color:MediumSeaGreen;\">Synthetic Data: Evaluating the quality (deferred)</font>\n",
    "\n",
    "[Synthetic data: Evaluation](SyntheticData_Evaluation.ipynb)\n",
    "- [case study: Evaluation of Time GAN](TimeGAN_evaluation.ipynb)\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "[Frechet Inception Distance (FID)](https://arxiv.org/pdf/1706.08500.pdf#page=39)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Social concerns (Week 7 early start)\n",
    "\n",
    "### Model bias\n",
    "- show model cards\n",
    "\n",
    "### Environmental\n",
    "\n",
    "Training a Large Language Model uses substantial compute, which means lots of energy and, hence,\n",
    "environmental impact.\n",
    "\n",
    "There is also a cost to *using* (i.e., inference) a trained model.\n",
    "\n",
    "Let's examine the impact.\n",
    "\n",
    "- [ML Carbon impact calculator](https://mlco2.github.io/impact/#compute)\n",
    "    - GPT-3:\n",
    "        - $\\approx 10^{23}$ Flops to train (see [Total compute: detailed calc for many models])(https://arxiv.org/pdf/2005.14165.pdf#page=46)\n",
    "        - NVidia A100: 5*10^{15} Flops\n",
    "        - 3600 seconds/hour\n",
    "        - $\\frac{10^{23}}{ 3600 * 5 * 10^{15}} \\approx 28000$ hours to train on single A100\n",
    "        - Calculated $\\text{CO}_2$ equivalent: $1.7* 10^4$ kilometers of car driving\n",
    "\n",
    "        - According to the [Scaling laws module](Index_Advanced.ipynb#Transformers:-Scaling)\n",
    "            - we could potentially achieve the same performance of GPT-3 on a *smaller* (less environmentally impactful model)\n",
    "    - [Energy: train vs inference](https://arxiv.org/pdf/2005.14165.pdf#page=39)\n",
    "### Alignment\n",
    "\n",
    "Language models show great capabilities but also the potential for harm: biased and offensive generated text, for example.  Can we \"align\" a model's output with human values ?\n",
    "\n",
    "- [Alignment](Alignment.ipynb)\n",
    "- [Alignment Anthropic](Alignment_Anthropics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7 Advanced topics\n",
    "\n",
    "Topics with a  <font style=\"background-color:MediumSeaGreen;\">Green Background</font>\n",
    "have been \"up-voted\" in a survey of students.\n",
    "\n",
    "The topics *not in green background* were not presented, due to lack of time\n",
    "- hopefully the notebooks are understandable on their own without in-class presentation\n",
    "\n",
    "## Social concerns\n",
    "\n",
    "### <font style=\"background-color:MediumSeaGreen;\">Alignment</font>\n",
    "\n",
    "Language models show great capabilities but also the potential for harm: biased and offensive generated text, for example.  Can we \"align\" a model's output with human values ?\n",
    "\n",
    "The Anthropic approach also uses Synthetic Data:\n",
    "- a partially-aligned model generates examples\n",
    "- to further align itself !\n",
    "\n",
    "\n",
    "- [Alignment (continued)](Alignment.ipynb#Reinforcement-Learning)\n",
    "- [Alignment Anthropic](Alignment_Anthropic.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- OpenAI\n",
    "    - [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)\n",
    "    - [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "- Anthropic\n",
    "    - [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf)\n",
    "        - [summary](https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3)\n",
    "    - [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)\n",
    "    - [Constitutional AI](https://arxiv.org/pdf/2212.08073.pdf)\n",
    "\n",
    "**Deeper Dive** \n",
    "\n",
    "- [Reinforcement Learning](Reinforcement%20Learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Privacy\n",
    "\n",
    "Each individual has \"sensitive\" personal data that would cause harm if shared.  Yet, this data also is a potentially valuable input for training models that could be beneficial to society.\n",
    "\n",
    "Is it possible to contribute sensitive data to a training dataset while limiting the potential for harm ?\n",
    "\n",
    "- [Extracting Training Data from an LLM](DL_Privacy.ipynb#)\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "- [Extracting Training Data from LLM](https://arxiv.org/pdf/2012.07805.pdf)\n",
    "- [TensorFlow Privacy](https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy)\n",
    "\n",
    "**Further reading**\n",
    "- [Differentially Private BERT](https://arxiv.org/pdf/2108.01624.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E: Mixing Text and Image\n",
    "\n",
    "We make use of the Quantized VAE technique we learned in the module on Autoencoders to enable us\n",
    "to mix text and image.  \n",
    "\n",
    "We discuss how a Text to Image model (convert the textual description of an image to an actual image) works.\n",
    "\n",
    "- [CLIP](CLIP.ipynb)\n",
    "    - [Zero shot learning, prompt engineering Colab notebook: PyTorch](https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)\n",
    "- [DALL-E](DALL-E.ipynb)\n",
    "- [Vision Transformer](Vision_Transformer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font style=\"background-color:MediumSeaGreen;\">Transformers: Scaling</font>\n",
    "\n",
    "We now have the capabilities to build models with extremely large number of weights.\n",
    "Is it possible to have too many weights ? \n",
    "\n",
    "Yes: weights, number of training examples and compute capacity\n",
    "combine to determine the performance of a model.\n",
    "\n",
    "There is an empirical result that suggests that in order to take advantage of GPT-3's use of 175 billion weights\n",
    "- 1000 times more compute is required than what was used\n",
    "- 10 times more training examples is required compared to what was used\n",
    "\n",
    "[How large should my Transformer be ?](Transformers_Scaling.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- [Scaling laws](https://arxiv.org/pdf/2001.08361.pdf)\n",
    "\n",
    "##  <font style=\"background-color:MediumSeaGreen;\">Combining a LLM with external capabilities</font>\n",
    "\n",
    "The module on Scaling argued that perhaps some models have *too many* parameters (given the size of the training dataset).\n",
    "Many model parameters probably encode \"world knowledge\" (facts) rather than \"task knowledge\" (how to solve a task). \n",
    "\n",
    "What if we allow world knowledge to be acquired at *inference time* by connecting the model to an external knowledge source (e.g., the Web) ?  The esult would be smaller models with more current knowledge.\n",
    "\n",
    "\n",
    "- [Extra parametric capabilities](LLM_plus_Extra_Parametric.ipynb)\n",
    "\n",
    "\n",
    "**Suggested reading**\n",
    "- [Retriever-Generator]([paper](https://arxiv.org/pdf/2005.11401.pdf))\n",
    "\n",
    "## Retriever-Generator examples: WebGPT, RETRO\n",
    "- [Non-parametric knowledge](Retriever_plus_LLM.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- [WebGPT](https://arxiv.org/abs/2112.09332)\n",
    "\n",
    "**Further reading**\n",
    "- [Sentence embeddings](https://arxiv.org/pdf/1908.10084.pdf))\n",
    "\n",
    "\n",
    "\n",
    "## Training: tricks of the trade\n",
    "\n",
    "Training, in practice, involves more than a model and a training set\n",
    "- Using multiple machines/GPU's: expect something to fail in the middle\n",
    "- Loss does not always decrease with increasing epoch\n",
    "    - Learning rate schedule \"mid-flight corrections\"\n",
    "\n",
    "Some practical lessons are found [here](Training_a_LLM_practical.ipynb).\n",
    "\n",
    "**Suggested reading**\n",
    "- [OPT: Open Pre Trained Model](https://arxiv.org/pdf/2205.01068.pdf)\n",
    "\n",
    "##  <font style=\"background-color:MediumSeaGreen;\">Limits of Transfer Learning</font>\n",
    "- [Limits of Transfer Learning](T5_Limits_of_Transfer_Learning.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "- [T5: Limits of Transfer Learning](https://arxiv.org/pdf/1910.10683.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n",
    "## Final Project\n",
    "\n",
    "[Assignment notebook](assignments/FineTuning_HF/FineTune_FinancialPhraseBank.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
