{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "$$\n",
       "\\newcommand{\\statf}{\\mathbf{s}} \n",
       "\n",
       "\\def\\timess#1#2{(#1:#2)}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "$$\n",
    "\\newcommand{\\statf}{\\mathbf{s}} \n",
    "\n",
    "\\def\\timess#1#2{(#1:#2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**References**\n",
    "\n",
    " [paper](https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n",
    "- [supplement](https://www.vanderschaar-lab.com/papers/NIPS2019_TGAN_Supplementary.pdf)\n",
    "- [github](https://github.com/vanderschaarlab/mlforhealthlabpub/tree/main/alg/timegan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "A GAN learns to produce synthetic (\"fake\") feature vectors $\\hat{\\x}$ of length $n$ that\n",
    "are plausible elements $\\x \\in \\pdata$.\n",
    "- The relationship between two features $\\hat{\\x}_,\\hat{\\x}_{j'}$ of synthetic $\\hat{\\x}$ should be consistent with the relations between $\\x_j, \\x_{j'}$ of real $\\x$\n",
    "- **cross sectional**\n",
    "    - consistency between the pixels of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But much data (especially in Finance) *also* has a **time** dimension\n",
    "- $\\x$ is two dimensional\n",
    "    - $\\x_\\tp$ is a vector of $n$ features, representing the state of the world at time $\\tt$\n",
    "\n",
    "\n",
    "The goal of a generative model would be to generate examples that are sequences (of length $T$) where each element is a vector of length $n$.\n",
    "- Each training example is a sequence $\\x_{(1)}, \\ldots \\x_{(T)}$\n",
    "- Generate synthetic $\\hat{\\x}_{(1)}, \\ldots \\hat{\\x}_{(T)}$\n",
    "- with both forms of consistency\n",
    "    - cross sectional between features at a fixed time $\\tt$: $\\hat{\\x}_{\\tp, j}$ and $\\hat{\\x}_{\\tp, j'}$\n",
    "    - sequential (\"time series\") between the feature vectors at different time steps: $\\hat{\\x}_\\tp$ and $\\hat{\\x}_{(\\tt+1)}$\n",
    "\n",
    "Following the notation of the paper\n",
    "- we use Python-like notation for sequences\n",
    "\n",
    "$$\n",
    "\\x_{\\timess{1}{T}} = \\x_{(1)}, \\ldots \\x_{(T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could try to encode a timeseries relationship as a pseudo cross-sectional relationship\n",
    "- flatten $(T \\times n)$ vector $\\x$ into a 1D vector of length $(T * n)$\n",
    "- the relationship between pairs of vector elements at distance $n$ would be one step of time\n",
    "\n",
    "This is unlikely to work well for common timeseries relationships: autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Time GAN (TGAN)* is a GAN (Generator/Discriminator pair) with an *extra* component: the *Supervisor*.\n",
    "\n",
    "The Supervisor is responsible for constraining the Generator to produce sequences with sequential properties\n",
    "\n",
    "- learns an autoregressive model of $\\x_{\\timess{1}{T}}$\n",
    "- creates a predicted $\\hat\\x_{\\timess{1}{T}}$\n",
    "    - creates a single element $\\hat\\x_\\tp$ at a time\n",
    "    - generating $\\hat\\x_\\tp$ conditional on $\\hat{\\x}_{\\timess{1}{\\tt-1}}$\n",
    "- creates a *Supervised Loss* which is added to the standard Generator Loss\n",
    "\n",
    "\n",
    "The Supervised Loss enforces the sequence dynamic as a constraint on one step ahead elements of the sequence.\n",
    "\n",
    "Thus, the Generator is encouraged to produce sequences that\n",
    "- not only fool the Discriminator\n",
    "- but also exhibit sequential properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is one additional component to the TGAN.\n",
    "\n",
    "Rather than having the Generator/Discriminator pair work on elements of $\\x_\\tp$\n",
    "- they both work on  reduced dimensional *encodings* of $\\x_\\tp$ denoted as $\\h_\\tp$\n",
    "- the reduced dimensional encoding  $\\h_\\tp$ is called an *embedding* of $\\x_\\tp$\n",
    "    - same idea as word embeddings\n",
    "\n",
    "The embedding is created by the *Embedder*\n",
    "- The Encoder half\n",
    "- of an Autoencoder (Encoder/Decoder pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This makes for a lot of moving parts (and a lot of notation).\n",
    "\n",
    "Each component contributes (at least one) one Loss to the total *multi-part* Loss.\n",
    "- Generator Loss\n",
    "- Discriminator Loss\n",
    "- Supervisor Loss\n",
    "- Autoencoder Reconstruction Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It also leads to options for how to train the components.\n",
    "\n",
    "Is the Autoencoder trained\n",
    "- independently\n",
    "- or jointly\n",
    "with the GAN + Supervisor components ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training scheme** (from paper)]\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Dh4JyfFTjkz6D2W4eDuMKg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "**Notation**\n",
    "\n",
    "text            | Description |\n",
    ":--|:--|\n",
    "$$\\statf$$ | static feature\n",
    "$$\\x_{\\timess{1}{T}}$$ | Temporal feature\n",
    "$$\\tilde\\x_{\\timess{1}{T}}$$ |  reconstructed (by Autoencoder) $\\x$\n",
    "| $\\tilde\\x_{\\timess{1}{T}} =$ `autoencoder`$(\\x_{\\timess{1}{T}})$\n",
    "$$\\hat{\\x}_{\\timess{1}{T}}$$ | fake data |\n",
    "| $\\hat{\\x}_{\\timess{1}{T}} = $ `decoder`$( \\hat\\h )$ |\n",
    "$$\\statf, \\x_{\\timess{1}{T}}$$ | real data -- s: non-sequence input, $x_{1:t}$ sequence input |\n",
    "$$\\tilde{\\statf}, \\tilde{\\x}_{\\timess{1}{T}}$$ | reconstructed real data (autoencoder output) |\n",
    "$$\\z_s, \\z_{\\timess{1}{T}}$$ | random vector (generator input) |\n",
    "$$\\h_s, \\h_{\\timess{1}{T}}$$ | real data in latent space (encoder output) |\n",
    "| $\\h_{\\timess{1}{T}} =$ `embedder`$(\\x_{\\timess{1}{T}})$ |\n",
    "$$\\hat\\e_s, \\hat\\e_{\\timess{1}{T}}$$ | fake data (generator output) in latent space (encoder output) |\n",
    "| $\\hat\\e_{\\timess{1}{T}} =$ `generator`$(\\z )$|\n",
    "$$\\hat\\h_s, \\hat\\h_{\\timess{1}{T}}$$ | fake data (generator + supervisor output) in latent space (encoder output) |\n",
    "| $\\hat\\h{\\timess{1}{T}} =$ `supervisor`$(\\hat\\e )$|\n",
    "$$\\hat{\\hat\\h}_s, \\hat{\\hat\\h}_{\\timess{1}{T}}$$ | fake data (real + supervisor output) in latent space (encoder output) |\n",
    "| $\\hat{\\hat\\h}_{\\timess{1}{T}} =$ `supervisor`$(\\h )$|\n",
    "$$\\y$$       | Boolean: Real/Fake Discriminator or real data |\n",
    "$$\\hat{\\y}$$ | Boolean: Real/Fake Discriminator on fake data (generator + supervisor output) |\n",
    "$$\\hat{\\hat{\\y}}$$ | Boolean: Real/Fake Discriminator on fake data (real + supervisor output) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Losses**\n",
    "\n",
    "name         | Definition &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Description |\n",
    ":---|:--|:--\n",
    "embedding_loss | $\\text{MSE}(\\x_{\\timess{1}{T}}, \\tilde\\x_{\\timess{1}{T}})$ | Autoencoder reconstruction error \n",
    "e_loss | $\\text{embedding_loss}^{0.5}  +  \\text{generator_loss}_\\text{supervised} $ | Embedder Loss\n",
    "| | = Autoencoder reconstruction error + one step ahead prediction error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples, real and synthetic\n",
    "\n",
    "To be fully general, the paper *also* allows examples to have 2 parts\n",
    "- static (non-sequence)\n",
    "- sequence\n",
    "\n",
    "Thus\n",
    "- a real example is a pair $\\statf, \\x_{\\timess{1}{T}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we are primarily concerned with the sequence properties of $\\x_{\\timess{1}{T}}$, \n",
    "we may omit $\\statf$ to simplify the presentation.\n",
    "\n",
    "We will assume that each element $\\x_\\tp$ of (real and fake) sequences $\\x_{\\timess{1}{T}}$ is a vector of length $n$\n",
    "- for example, the time $\\tt$ characteristics of each of $n$ tickers\n",
    "\n",
    "We don't need to assume that $\\statf$ is also a vector of length $n$\n",
    "- e.g., no need to assume a unique characteristic per ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are *two kinds* of synthetic sequences (both sequences of embeddings rather than raw inputs)\n",
    "- $\\hat{\\h}_{\\timess{1}{T}}$: produced by the Supervisor, given input from the Generator\n",
    "\n",
    "$$\n",
    "\\hat{\\h}_{\\timess{1}{T}} = \\text{supervisor}( \\text{generator}(\\z) )\n",
    "$$\n",
    "\n",
    "- $\\hat{\\hat{\\h}}_{\\timess{1}{T}}$: produced by the Supervisor, given (the embedding of) **real** data $\\x$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\h_{\\timess{1}{T}} & = & \\text{embedder}(\\x_{\\timess{1}{T}}) \\\\\n",
    "\\hat{\\hat{\\h}}_{\\timess{1}{T}} & = & \\text{supervisor}( \\h_{\\timess{1}{T}}) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions: High level view of objectives\n",
    "\n",
    "The *Time GAN (TGAN)* tries to achieve both cross sectional and sequence objectives by a multi-part Loss function.\n",
    "\n",
    "The first objective (enforced by a loss function) is the familiar GAN objective\n",
    "- $\\pmodel \\approx \\pdata$ for some definition of equality of distributions\n",
    "- Also called: the *Unsupervised Loss*\n",
    "\n",
    "The second objective is sequence related\n",
    "- *Conditional* (one step ahead) distributions of Fake and Real are equal\n",
    "- $\\pmodel( \\x_\\tp | \\x_{\\timess{1}{\\tt-1}} ) \\approx \\pdata( \\x_\\tp | \\x_{\\timess{1}{\\tt-1}} )$\n",
    "- Also called: the *Supervised Loss*\n",
    "\n",
    "We will use the KL Divergence as our measure of \"dissimilarity\" of two distributions\n",
    "- Just like in the plain GAN: \n",
    "    - under the assumption that the optimal Discriminator can be found, the KL Divergence turns into the JSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we shall see\n",
    "- there are multiple steps to training the TGAN\n",
    "    - independent training of the Embedder\n",
    "    - independent training of the Supervisor\n",
    "    - joint training of all components\n",
    "    \n",
    "Thus, there will be a separate Loss function for each training step.\n",
    "\n",
    "The Losses will be multi-part: consisting of sums of terms for sub-losses.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "We use `BCE` as a function that computes *Binary Cross Entropy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedder (Autoencoder)\n",
    "\n",
    "Rather than working on \"raw\" examples $\\statf, \\x_{\\timess{1}{T}}$ the model creates an *embedding* of the example\n",
    "- lower dimensional representation\n",
    "- that preserves \"semantics\"\n",
    "    - same idea as embedding words, that is, changing the representation of a word\n",
    "        - from a categorical encoded by a *sparse*, very long OHE vector\n",
    "        - to a shorter, *dense* vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The embedding will be created by an Autoencoder\n",
    "- Given input example \n",
    "- Pass it through an Encoder: a bottle-neck that reduces the dimensions\n",
    "- Have the Decoder try to reconstruct the input, given the reduced dimension representation\n",
    "\n",
    "So the Embedder is implemented by the Encoder half of an Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder\n",
    "- takes input $\\statf, \\x_{\\timess{1}{T}}$\n",
    "- outputs reduced dimension embedding/latent $\\h_s, \\h_{\\timess{1}{T}}$\n",
    "\n",
    "The Decoder\n",
    "- takes input $\\h_s, \\h_{\\timess{1}{T}}$\n",
    "- outputs $\\tilde\\statf, \\tilde\\x_{\\timess{1}{T}}$\n",
    "The Encoder/Decoder pair is trained with the objective\n",
    "$$\n",
    "\\tilde\\statf, \\tilde\\x_{\\timess{1}{T}} \\approx \\statf, \\x_{\\timess{1}{T}}\n",
    "$$\n",
    "    \n",
    "That is: the Decoder attempts to create the best reconstruction of the Encoder input, given the embedding.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both the Encoder and Decoder \n",
    "- must be autoregressive\n",
    "    - generate embedding of future element, conditional on the past elements\n",
    "- must obey *causal ordering* of the sequence\n",
    "    - can't look at any of $\\x_{\\timess{t+1}{}}$ when embedding $\\x_\\tp$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use embeddings rather than raw data ?\n",
    "\n",
    "One obvious reason is space considerations\n",
    "- Having the GAN work with lower dimensional embeddings rather than high dimensional raw input\n",
    "\n",
    "The less obvious (perhaps) reason comes from our experience with sequence data such as time series of returns\n",
    "- PCA analysis shows that highly reduced dimensions preserve much of the variation\n",
    "- Factor models propose that a small number of common factors are responsible for the variation of the cross section of stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoencoder training (independent pass)\n",
    "\n",
    "There are two obvious choices for training the Autoencoder\n",
    "- Independent of the GAN Generator/Discriminator + Supervisor\n",
    "    - That is: just \"learn\" how to compress sequences without worrying about where the sequences come from\n",
    "- Jointly with the GAN + Supervisor   \n",
    "\n",
    "The authors chose\n",
    "- an initial training of the Autoencoder independently\n",
    "- followed by a joint training with the GAN + Supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the independent training of the Autoencoder we use the\n",
    "standard Reconstruction Loss of an Autoencoder:\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\loss_R & = & \n",
    "\\text{embedding_loss} & = & \\text{MSE}(\\x_{\\timess{1}{T}}, \\tilde\\x_{\\timess{1}{T}}) & \\text{Reconstruction Loss} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**This is the only Loss Term** that is optimized in the independent training.\n",
    "\n",
    "We defer the Autoencoder loss term that arises from joint training until after we introduce the Supervisor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervisor\n",
    "\n",
    "The Supervisor is a Recurrent Network that \n",
    "helps in learning an autoregressive model of $\\x_{\\timess{1}{T}}$.\n",
    "\n",
    "However: it works on the embeddings of data rather than the raw data\n",
    "- creates a predicted $\\h_{\\timess{1}{T}}$\n",
    "    - creates a single element $\\h_\\tp$ at a time\n",
    "    - generating $\\h_\\tp$ conditional on $\\h_{\\timess{1}{\\tt-1}}$\n",
    "- rather than creating a predicted $\\x_{\\timess{1}{T}}$\n",
    "    - if desired, can map the predicted $\\h_{\\timess{1}{T}}$ to predicted $\\x_{\\timess{1}{T}}$ using the Decoder half of the Autoencoder\n",
    "    $$\n",
    "    \\hat{\\x}_{\\timess{1}{T}} = \\text{decoder}( \\hat\\h )\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two sources of embeddings that are fed to the Supervisor.\n",
    "\n",
    "The Supervisor takes the given sequence of embeddings into the autoregressive sequence.\n",
    "\n",
    "The two inputs to the Supervisor, and their outputs are\n",
    "- Embeddings of real examples $\\x$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\h_{\\timess{1}{T}} & = & \\text{embedder}(\\x_{\\timess{1}{T}}) \\\\\n",
    "\\hat{\\hat\\h}_{\\timess{1}{T}} & = & \\text{supervisor} (\\h ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Embeddings created by the Generator (i.e., fake examples, in latent space)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat\\h{\\timess{1}{T}}  & = & \\text{supervisor}(\\hat\\e )\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervisor training (independent pass)\n",
    "\n",
    "There is a *Supervised Loss* associated with the Supervisor.\n",
    "\n",
    "It is a measure of the quality of the embedding as input to the Supervisor\n",
    "- Given the embedding, can the Supervisor construct a loss-less synthetic\n",
    "    - Note: this synthetic is derived from a **real** example $\\x$, **independent** of the Generator\n",
    "    \n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\h_{\\timess{1}{T}} & = & \\text{embedder}(\\x_{\\timess{1}{T}}) \\\\\n",
    "\\hat{\\hat\\h}_{\\timess{1}{T}} & = & \\text{supervisor}( \\h ) \\\\\n",
    "\\loss_S & = & \n",
    "\\text{generator_loss}_\\text{supervised} & = & \\text{MSE}( \\h_{\\timess{1}{}}, \\hat{\\hat\\h}_{\\timess{1}{T}}) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, $\\loss_S$ depends on the behavior of the Encoder side (`embedder`) of the Autoencoder.\n",
    "\n",
    "But just as we had an initial independent training of the Autoencoder\n",
    "- we have an initial independent training of the Superviser\n",
    "\n",
    "During joint training, we will use Loss functions that cause the weights\n",
    "of the `embedder` and `supervisor` to update together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminator\n",
    "\n",
    "There are three sources of examples fed to the Discriminator, each resulting\n",
    "in a different judgment of Real/Fake.\n",
    "\n",
    "Recall that the Generator, Discriminator and Supervisor all take embeddings (as opposed to raw examples) as input.\n",
    "\n",
    "The three inputs to the Discriminator, their outputs (judgments), and associated losses are\n",
    "- Embeddings of Real examples\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\h_{\\timess{1}{T}} & = & \\text{embedder}(\\x_{\\timess{1}{T}}) \\\\\n",
    "\\y & = &  \\text{discriminator}(\\h) \\\\\n",
    "\\text{D_loss_real} & = & \\text{BCE}(1's,  \\y) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator only)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat{\\y} & = & \\y_\\text{fake_e}  =  \\text{discriminator}( \\hat\\e) \\\\\n",
    "\\text{D_loss_fake_e} &  = & \\text{BCE}(0's, \\y_\\text{fake_e}) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised_e} & = &  \\text{BCE}( 1's, \\y_\\text{fake_e} ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator + Supervisor)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat\\h{\\timess{1}{T}}  & = & \\text{supervisor}(\\hat\\e ) \\\\\n",
    "\\dot\\y & = & \\y_\\text{fake}  =  \\text{discriminator}( \\hat\\h ) \\\\\n",
    "\\text{D_loss_fake} &  = & \\text{BCE}(0's, \\y_\\text{fake}) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised} &  = & \\text{BCE}(1's, \\y_\\text{fake})\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Discriminator Loss $\\loss_D$ is the (weighted, but we ignore the weights) sum\n",
    "$$\n",
    "\\loss_D = \\text{D_loss_real} + \\text{D_loss_unsupervised} + \\text{D_loss_fake}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generator\n",
    "\n",
    "**Remark and warning**\n",
    "\n",
    "There are a number of variables in \n",
    "[Janssen's implementation of TimeGAN](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/02_TimeGAN_TF2.ipynb)\n",
    "that begin with `generator_loss`.\n",
    "\n",
    "I will use these variable names in the presentation below (even though I find them somewhat confusing).\n",
    "\n",
    "- `generator_loss_supervised` refers to the loss of the Supervisor: $\\loss_S$\n",
    "    - I would have chosen: `supervisor_loss` rather than `generator_loss_supervised`\n",
    "- There are two variable names beginning with `generator_loss_unsupervised`\n",
    "    - Unfortunately, at least one of them references the output of the Supervisor, which is confusing\n",
    "        - `generator_loss_unsupervised` is the loss associated with examples created by the Generator **and** the Supervisor\n",
    "            - I might have chosen `generator_loss_with_sup`\n",
    "        - `generator_loss_unsupervised_e` is the loss associated with examples created by the Generator alone (without the Supervisor)\n",
    "            - `generator_loss_without_sup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generator\n",
    "\n",
    "\n",
    "The Generator tries to fool the Discriminator in two ways.\n",
    "- via direct output of the Generator\n",
    "- by the output of the Supervisor\n",
    "    - creates an autoregressive model of the direct output of the Generator\n",
    "    \n",
    "Recall that the Generator, Discriminator and Supervisor all take embeddings (as opposed to raw examples) as input.\n",
    "\n",
    "The two outputs of the Generator, their evaluation by the Discriminator, and associated losses ar\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator only)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat{\\y} & = & \\y_\\text{fake_e}  =  \\text{discriminator}( \\hat\\e) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised_e} & = &  \\text{BCE}( 1's, \\y_\\text{fake_e} ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator + Supervisor)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat\\h_{\\timess{1}{T}}  & = & \\text{supervisor}(\\hat\\e ) \\\\\n",
    "\\dot\\y & = & \\y_\\text{fake}  =  \\text{discriminator}( \\hat\\h ) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised} &  = & \\text{BCE}(1's, \\y_\\text{fake})\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is an *additional* loss term associated with the Generator.\n",
    "\n",
    "It compares the statistical moments (mean, variance) of the\n",
    "real examples with fake examples\n",
    "- real examples with first two moments $\\mu, \\sigma$\n",
    "- fake examples with first two moments $\\hat\\mu, \\hat\\sigma$\n",
    "    - fake examples obtained from the sequence of embeddings $\\hat\\h$ produced by (Generator + Supervisor) using the Decoder side of the Autoencoder\n",
    "    $$\n",
    "\\hat{\\x}_{\\timess{1}{T}} = \\text{decoder}( \\hat\\h_{\\timess{1}{T}} )\n",
    "$$\n",
    "\n",
    "The Generator Moments loss is\n",
    "$$    \n",
    "\\text{generator_loss}_\\text{moment}  =  (\\mu - \\hat{\\mu}) + (\\sigma - \\hat{\\sigma} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The total loss $\\loss_G$ for the Generator is the sum of the sub-losses\n",
    "\n",
    "$$\n",
    "\\loss_G = \\text{generator_loss}_\\text{unsupervised_e} + \\text{generator_loss}_\\text{unsupervised} + \n",
    "\\loss_S + \\text{generator_loss}_\\text{moment}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Joint training\n",
    "\n",
    "The Embedder and Supervisor are initially trained (separately) independently of the GAN.\n",
    "\n",
    "It is during joint training that the GAN (Generator and Discriminator) are trained, in the usual Adversarial Training manner of a GAN.\n",
    "\n",
    "It may not be obvious but, even though Adversarial Training looks like it is designed to update\n",
    "the Generator and Discriminator weights\n",
    "- all weights are potentially updated, including the Embedder and Supervisor\n",
    "\n",
    "There is a bit of subtlety here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most obvious purpose of joint training is to implement the GAN adversarial training\n",
    "- Generator weights $\\Theta_G$ are updated to better produce examples to fool the Discriminator\n",
    "- Discriminator weights $\\Theta_D$ are updated to better distinguish between Real and Fake examples.\n",
    "\n",
    "How do the weights of the Supervisor and Autoencoder come into play ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that the Generator creates two kinds of Fake embeddings of examples.\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator only)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat{\\y} & = & \\y_\\text{fake_e}  =  \\text{discriminator}( \\hat\\e) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised_e} & = &  \\text{BCE}( 1's, \\y_\\text{fake_e} ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Embeddings of Fake examples (created by Generator + Supervisor)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat\\h_{\\timess{1}{T}}  & = & \\text{supervisor}(\\hat\\e ) \\\\\n",
    "\\dot\\y & = & \\y_\\text{fake}  =  \\text{discriminator}( \\hat\\h ) \\\\\n",
    "\\text{generator_loss}_\\text{unsupervised} &  = & \\text{BCE}(1's, \\y_\\text{fake})\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first \n",
    "- $\\hat\\e_{\\timess{1}{T}}  =  \\text{generator} (\\z )$\n",
    "\n",
    "is independent of the Supervisor, but the second\n",
    "- $\\hat\\h_{\\timess{1}{T}}   = \\text{supervisor}(\\hat\\e )$\n",
    "is affected by the Supervisor (hence its weights).\n",
    "\n",
    "Moreover, the second kind of example is fed into the Discriminator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, in order to reduce the associated Generator loss\n",
    "$$\n",
    "\\text{generator_loss}_\\text{unsupervised}   =  \\text{BCE}(1's, \\y_\\text{fake})\n",
    "$$\n",
    "or associated Discriminator Loss\n",
    "\n",
    "$$\n",
    "\\text{D_loss_fake}   =  \\text{BCE}(0's, \\y_\\text{fake}) \n",
    "$$\n",
    "\n",
    "a gradient may arise that affects the weights of the Supervisor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the Supervisor Loss does not depend *directly* on Fake examples.\n",
    "\n",
    "It is defined solely on real examples\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\h_{\\timess{1}{T}} & = & \\text{embedder}(\\x_{\\timess{1}{T}}) \\\\\n",
    "\\hat{\\hat\\h}_{\\timess{1}{T}} & = & \\text{supervisor}( \\h ) \\\\\n",
    "\\loss_S & = & \n",
    "\\text{generator_loss}_\\text{supervised} & = & \\text{MSE}( \\h_{\\timess{1}{}}, \\hat{\\hat\\h}_{\\timess{1}{T}}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "It is not exposed directly to fake examples, only indirectly\n",
    "- Through the dependence of part of the Generator and Disciminator losses on the Supervisor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It makes sense that $\\loss_S$ depends only on real examples\n",
    "- they are from $\\pdata$, which defines all statistical relationships\n",
    "- we can't infer any true relationship from Fake data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Adversarial Training of the GAN can modify $\\loss_S$ which depends on the Encoder half of the Autoencoder\n",
    "- see the equations above\n",
    "    - $\\loss_S$ depends on $\\h_{\\timess{1}{T}}$\n",
    "    - $\\h_{\\timess{1}{T}} = \\text{embedder}(\\x_{\\timess{1}{T}})$\n",
    "   \n",
    "Hence Adversarial Training can also affect the weights of the Autoencoder.   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Thus, the Autoencoder learns a better encoding by joint training.\n",
    "\n",
    "\n",
    "We can see the definition of `e_loss` combines these the two \"reconstruction\" related terms\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\tilde\\x_{\\timess{1}{\\tt}} & = & \\text{autoencoder}(\\x_{\\timess{1}{T}}) & \\text{Reconstructed } \\x\\\\\n",
    "\\text{embedding_loss} & = & \\text{MSE}(\\x_{\\timess{1}{T}}, \\tilde\\x_{\\timess{1}{T}}) & \\text{Reconstruction error} \\\\\n",
    "& = &  \\text{embedding_loss}^{0.5}  + \\text{generator_loss}_\\text{supervised} \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fact that the Embedder and Supervisor/Generator/Discriminator are tied together in training\n",
    "is also apparent if we zoom in on the diagram of the Training Scheme\n",
    "\n",
    "<table>\n",
    "    \n",
    "<tr><th>Training Scheme: zoom in on Embedder gradient updates</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <img src=\"images/TGAN_training_Autoencoder.png\">\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Notice: the gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss_S}{\\partial \\Theta_e}\n",
    "$$\n",
    "\n",
    "of the Supervised Loss with respect to the Encoder weights $\\Theta_e$ flowing back to the Encoder $e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generating synthetic examples\n",
    "\n",
    "Once trained, we can generate new Fake examples\n",
    "\n",
    "- by creating a random noise vector $\\z$\n",
    "- feeding $\\z$ to the `generator` and `supervisor` to get a synthetic embedding (created by Generator + Supervisor)\n",
    "- decoding the synthetic embedding (by the Decoder part of the Autoencoder) to create a sequence in the original data space\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\e_{\\timess{1}{T}} & = & \\text{generator} (\\z ) \\\\\n",
    "\\hat\\h_{\\timess{1}{T}}  & = & \\text{supervisor}(\\hat\\e ) \\\\\n",
    "\\hat{\\x}_{\\timess{1}{T}} & = & \\text{decoder}( \\hat\\h_{\\timess{1}{T}} ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code\n",
    "\n",
    "There is an implementation of TimeGAN in Stefan Janssen's [ML4T Chapt 21](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/02_TimeGAN_TF2.ipynb)\n",
    "which is derived from the [author's implementation](https://github.com/vanderschaarlab/mlforhealthlabpub/blob/main/alg/timegan/tgan.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
